{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transliteration Model - Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining our custom accuracy function `custom_sparse_categorical_accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "def custom_sparse_categorical_accuracy(y_true, y_pred):\n",
    "    return K.cast(K.equal(K.max(y_true, axis=-1),\n",
    "                          K.cast(K.argmax(y_pred, axis=-1), K.floatx())),\n",
    "                  K.floatx())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('transliteration_model.h5', custom_objects={'custom_sparse_categorical_accuracy': custom_sparse_categorical_accuracy})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Pre-Processing Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def tokenize(x):\n",
    "    \"\"\"\n",
    "    Tokenize x\n",
    "    :param x: List of sentences/strings to be tokenized\n",
    "    :return: Tuple of (tokenized x data, tokenizer used to tokenize x)\n",
    "    \"\"\"\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    return tokenizer.texts_to_sequences(x), tokenizer\n",
    "\n",
    "def pad(x, length=None):\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    return pad_sequences(x, maxlen=length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import helper\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "\n",
    "english_words = helper.load_data('data/neural_english.txt')\n",
    "bengali_words = helper.load_data('data/neural_bengali.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x, y):\n",
    "    \"\"\"\n",
    "    Preprocess x and y\n",
    "    :param x: Feature List of sentences\n",
    "    :param y: Label List of sentences\n",
    "    :return: Tuple of (Preprocessed x, Preprocessed y, x tokenizer, y tokenizer)\n",
    "    \"\"\"\n",
    "    preprocess_x, x_tk = tokenize(x)\n",
    "    preprocess_y, y_tk = tokenize(y)\n",
    "\n",
    "    preprocess_x = pad(preprocess_x)\n",
    "\n",
    "    return preprocess_x, x_tk, y_tk\n",
    "\n",
    "preproc_bengali_words, bengali_tokenizer, english_tokenizer =\\\n",
    "    preprocess(bengali_words, english_words)\n",
    "\n",
    "max_bengali_sequence_length = preproc_bengali_words.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_preprocess(word, tokenizer, sequence_length):\n",
    "    \"\"\"\n",
    "    Preprocess the input\n",
    "    Return : Preprocessed input and the Tokenizer\n",
    "    \"\"\"\n",
    "    preprocess_x = tokenizer.texts_to_sequences(word)\n",
    "    \n",
    "    result = [0 for i in range(sequence_length)]\n",
    "    \n",
    "    k = 0\n",
    "    \n",
    "    for i in range(len(preprocess_x)):\n",
    "        try :\n",
    "            result[k] = preprocess_x[i][0]\n",
    "            k = k + 1\n",
    "        except :\n",
    "            pass\n",
    "    \n",
    "    return np.array([result]).astype('int32')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logits_to_text(logits, tokenizer):\n",
    "    \"\"\"\n",
    "    Turn logits from a neural network into text using the tokenizer\n",
    "    :param logits: Logits from a neural network\n",
    "    :param tokenizer: Keras Tokenizer fit on the labels\n",
    "    :return: String that represents the text of the logits\n",
    "    \"\"\"\n",
    "    index_to_words = {id: word for word, id in tokenizer.word_index.items()}\n",
    "    index_to_words[0] = ' '\n",
    "\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_processed_bengali_word(bengali_word):\n",
    "    result = \"\"\n",
    "    for i in bengali_word:\n",
    "        result = result + i + \" \"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(bengali_word, bengali_tokenizer, max_seq_length, english_tokenizer):\n",
    "    p_word = pre_processed_bengali_word(bengali_word)\n",
    "    preproc_input = input_preprocess(p_word, bengali_tokenizer, max_seq_length)\n",
    "    prediction = logits_to_text(model.predict(preproc_input)[0], english_tokenizer)\n",
    "    \n",
    "    result = \"\"\n",
    "    for i in prediction.strip():\n",
    "        if i != ' ':\n",
    "            result = result + i.upper()\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving necessary files for easy reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"ben_token.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(bengali_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open(\"max_seq_length.txt\", \"w\") as fp:\n",
    "    fp.write(str(max_bengali_sequence_length))\n",
    "\n",
    "with open(\"eng_token.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(english_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
